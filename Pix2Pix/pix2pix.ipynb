{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder_block(nn.Module):\n",
    "    def __init__(self, in_channels, num_filters, batch_size, activ=nn.LeakyReLU, norm=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        if norm is True:\n",
    "            normalization = None\n",
    "            if batch_size == 1:\n",
    "                normalization = nn.InstanceNorm2d(num_filters)\n",
    "            else:\n",
    "                normalization = nn.BatchNorm2d(num_filters)\n",
    "            \n",
    "            self.layers = nn.Sequential(\n",
    "                activ(0.2, inplace=True),\n",
    "                nn.Conv2d(in_channels, num_filters, kernel_size=4, stride=2, padding=1),\n",
    "                normalization\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            self.layers = nn.Sequential(\n",
    "                activ(0.2, inplace=True),\n",
    "                nn.Conv2d(in_channels, num_filters, kernel_size=4, stride=2, padding=1)\n",
    "            )\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = self.layers(X)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder_block(nn.Module):\n",
    "    def __init__(self, in_channels, num_filters, batch_size, activ=nn.ReLU, dropout=True, attach=True):\n",
    "        super().__init__()\n",
    "\n",
    "        normalization = None\n",
    "        if batch_size == 1:\n",
    "            normalization = nn.InstanceNorm2d(num_filters)\n",
    "        else:\n",
    "            normalization = nn.BatchNorm2d(num_filters)\n",
    "\n",
    "        if dropout is True:\n",
    "            drop = nn.Dropout(0.5)\n",
    "            if attach is True:\n",
    "                self.layers = nn.Sequential(\n",
    "                    activ(inplace=True),\n",
    "                    nn.ConvTranspose2d(in_channels*2, num_filters, kernel_size=4, stride=2, padding=1),\n",
    "                    normalization,\n",
    "                    drop\n",
    "                )\n",
    "            else:\n",
    "                self.layers = nn.Sequential(\n",
    "                activ(inplace=True),\n",
    "                nn.ConvTranspose2d(in_channels, num_filters, kernel_size=4, stride=2, padding=1),\n",
    "                normalization,\n",
    "                drop\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.layers = nn.Sequential(\n",
    "                activ(inplace=True),\n",
    "                nn.ConvTranspose2d(in_channels*2, num_filters, kernel_size=4, stride=2, padding=1),\n",
    "                normalization\n",
    "            )\n",
    "\n",
    "    def forward(self, X, encoder):\n",
    "        out = self.layers(X)\n",
    "        out = torch.cat((encoder,out), dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, batch_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Batch-Norm is not applied to the first C64 layer in the encoder\n",
    "        self.conv1 = nn.Conv2d(3, 64, 4, 2, 1)\n",
    "        self.conv2 = encoder_block(64, 128, batch_size=batch_size)\n",
    "        self.conv3 = encoder_block(128, 256, batch_size=batch_size)\n",
    "        self.conv4 = encoder_block(256, 512, batch_size=batch_size)\n",
    "        self.conv5 = encoder_block(512, 512, batch_size=batch_size)\n",
    "        self.conv6 = encoder_block(512, 512, batch_size=batch_size)\n",
    "        self.conv7 = encoder_block(512, 512, batch_size=batch_size)\n",
    "        self.conv8 = encoder_block(512, 512, batch_size=batch_size, norm=False)\n",
    "\n",
    "        self.deconv8 = decoder_block(512,512, batch_size=batch_size, attach=False)\n",
    "        self.deconv7 = decoder_block(512,512, batch_size=batch_size)\n",
    "        self.deconv6 = decoder_block(512,512, batch_size=batch_size)\n",
    "        self.deconv5 = decoder_block(512,512, batch_size=batch_size, dropout=False)\n",
    "        self.deconv4 = decoder_block(512,256, batch_size=batch_size, dropout=False)\n",
    "        self.deconv3 = decoder_block(256,128, batch_size=batch_size, dropout=False)\n",
    "        self.deconv2 = decoder_block(128,64, batch_size=batch_size, dropout=False)\n",
    "        self.deconv1 = nn.Sequential(\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64 * 2, 3, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        x1 = self.conv1(X)\n",
    "        x2 = self.conv2(x1)\n",
    "        x3 = self.conv3(x2)\n",
    "        x4 = self.conv4(x3)\n",
    "        x5 = self.conv5(x4)\n",
    "        x6 = self.conv6(x5)\n",
    "        x7 = self.conv7(x6)\n",
    "        x8 = self.conv8(x7)\n",
    "\n",
    "        out = self.deconv8(x8, x7)\n",
    "        out = self.deconv7(out, x6)\n",
    "        out = self.deconv6(out, x5)\n",
    "        out = self.deconv5(out, x4)\n",
    "        out = self.deconv4(out, x3)\n",
    "        out = self.deconv3(out, x2)\n",
    "        out = self.deconv2(out, x1)\n",
    "        out = self.deconv1(out)\n",
    "    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand((1,3,256,256))\n",
    "gen = Generator(batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 128, 128]           3,136\n",
      "         LeakyReLU-2         [-1, 64, 128, 128]               0\n",
      "            Conv2d-3          [-1, 128, 64, 64]         131,200\n",
      "    InstanceNorm2d-4          [-1, 128, 64, 64]               0\n",
      "     encoder_block-5          [-1, 128, 64, 64]               0\n",
      "         LeakyReLU-6          [-1, 128, 64, 64]               0\n",
      "            Conv2d-7          [-1, 256, 32, 32]         524,544\n",
      "    InstanceNorm2d-8          [-1, 256, 32, 32]               0\n",
      "     encoder_block-9          [-1, 256, 32, 32]               0\n",
      "        LeakyReLU-10          [-1, 256, 32, 32]               0\n",
      "           Conv2d-11          [-1, 512, 16, 16]       2,097,664\n",
      "   InstanceNorm2d-12          [-1, 512, 16, 16]               0\n",
      "    encoder_block-13          [-1, 512, 16, 16]               0\n",
      "        LeakyReLU-14          [-1, 512, 16, 16]               0\n",
      "           Conv2d-15            [-1, 512, 8, 8]       4,194,816\n",
      "   InstanceNorm2d-16            [-1, 512, 8, 8]               0\n",
      "    encoder_block-17            [-1, 512, 8, 8]               0\n",
      "        LeakyReLU-18            [-1, 512, 8, 8]               0\n",
      "           Conv2d-19            [-1, 512, 4, 4]       4,194,816\n",
      "   InstanceNorm2d-20            [-1, 512, 4, 4]               0\n",
      "    encoder_block-21            [-1, 512, 4, 4]               0\n",
      "        LeakyReLU-22            [-1, 512, 4, 4]               0\n",
      "           Conv2d-23            [-1, 512, 2, 2]       4,194,816\n",
      "   InstanceNorm2d-24            [-1, 512, 2, 2]               0\n",
      "    encoder_block-25            [-1, 512, 2, 2]               0\n",
      "        LeakyReLU-26            [-1, 512, 2, 2]               0\n",
      "           Conv2d-27            [-1, 512, 1, 1]       4,194,816\n",
      "    encoder_block-28            [-1, 512, 1, 1]               0\n",
      "             ReLU-29            [-1, 512, 1, 1]               0\n",
      "  ConvTranspose2d-30            [-1, 512, 2, 2]       4,194,816\n",
      "   InstanceNorm2d-31            [-1, 512, 2, 2]               0\n",
      "          Dropout-32            [-1, 512, 2, 2]               0\n",
      "    decoder_block-33           [-1, 1024, 2, 2]               0\n",
      "             ReLU-34           [-1, 1024, 2, 2]               0\n",
      "  ConvTranspose2d-35            [-1, 512, 4, 4]       8,389,120\n",
      "   InstanceNorm2d-36            [-1, 512, 4, 4]               0\n",
      "          Dropout-37            [-1, 512, 4, 4]               0\n",
      "    decoder_block-38           [-1, 1024, 4, 4]               0\n",
      "             ReLU-39           [-1, 1024, 4, 4]               0\n",
      "  ConvTranspose2d-40            [-1, 512, 8, 8]       8,389,120\n",
      "   InstanceNorm2d-41            [-1, 512, 8, 8]               0\n",
      "          Dropout-42            [-1, 512, 8, 8]               0\n",
      "    decoder_block-43           [-1, 1024, 8, 8]               0\n",
      "             ReLU-44           [-1, 1024, 8, 8]               0\n",
      "  ConvTranspose2d-45          [-1, 512, 16, 16]       8,389,120\n",
      "   InstanceNorm2d-46          [-1, 512, 16, 16]               0\n",
      "    decoder_block-47         [-1, 1024, 16, 16]               0\n",
      "             ReLU-48         [-1, 1024, 16, 16]               0\n",
      "  ConvTranspose2d-49          [-1, 256, 32, 32]       4,194,560\n",
      "   InstanceNorm2d-50          [-1, 256, 32, 32]               0\n",
      "    decoder_block-51          [-1, 512, 32, 32]               0\n",
      "             ReLU-52          [-1, 512, 32, 32]               0\n",
      "  ConvTranspose2d-53          [-1, 128, 64, 64]       1,048,704\n",
      "   InstanceNorm2d-54          [-1, 128, 64, 64]               0\n",
      "    decoder_block-55          [-1, 256, 64, 64]               0\n",
      "             ReLU-56          [-1, 256, 64, 64]               0\n",
      "  ConvTranspose2d-57         [-1, 64, 128, 128]         262,208\n",
      "   InstanceNorm2d-58         [-1, 64, 128, 128]               0\n",
      "    decoder_block-59        [-1, 128, 128, 128]               0\n",
      "             ReLU-60        [-1, 128, 128, 128]               0\n",
      "  ConvTranspose2d-61          [-1, 3, 256, 256]           6,147\n",
      "             Tanh-62          [-1, 3, 256, 256]               0\n",
      "================================================================\n",
      "Total params: 54,409,603\n",
      "Trainable params: 54,409,603\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 140.62\n",
      "Params size (MB): 207.56\n",
      "Estimated Total Size (MB): 348.93\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(gen, (3,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, batch_size):\n",
    "        super().__init__()\n",
    "\n",
    "        normalization = None\n",
    "        if batch_size == 1:\n",
    "            normalization = nn.InstanceNorm2d\n",
    "        else:\n",
    "            normalization = nn.BatchNorm2d\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(3*2, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            normalization(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            normalization(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=1),\n",
    "            normalization(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, real, fake):\n",
    "        out = torch.cat((real, fake), dim=1)\n",
    "        out = self.layers(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis = Discriminator(batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 128, 128]           6,208\n",
      "         LeakyReLU-2         [-1, 64, 128, 128]               0\n",
      "            Conv2d-3          [-1, 128, 64, 64]         131,200\n",
      "    InstanceNorm2d-4          [-1, 128, 64, 64]               0\n",
      "         LeakyReLU-5          [-1, 128, 64, 64]               0\n",
      "            Conv2d-6          [-1, 256, 32, 32]         524,544\n",
      "    InstanceNorm2d-7          [-1, 256, 32, 32]               0\n",
      "         LeakyReLU-8          [-1, 256, 32, 32]               0\n",
      "            Conv2d-9          [-1, 512, 31, 31]       2,097,664\n",
      "   InstanceNorm2d-10          [-1, 512, 31, 31]               0\n",
      "        LeakyReLU-11          [-1, 512, 31, 31]               0\n",
      "           Conv2d-12            [-1, 1, 30, 30]           8,193\n",
      "          Sigmoid-13            [-1, 1, 30, 30]               0\n",
      "================================================================\n",
      "Total params: 2,767,809\n",
      "Trainable params: 2,767,809\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 147456.00\n",
      "Forward/backward pass size (MB): 45.28\n",
      "Params size (MB): 10.56\n",
      "Estimated Total Size (MB): 147511.83\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(dis, [(3,256,256),(3,256,256)])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
